import { supabase, supabaseAdmin } from './supabase'
import { readSheetDataDynamic } from './googleSheets'
import { highPerformanceCache, databaseOptimizer } from './performanceOptimizer'

// ìµœì í™”ëœ ë™ê¸°í™” ì„œë¹„ìŠ¤
export class OptimizedSyncService {
  private static instance: OptimizedSyncService
  private cache = new Map<string, { data: any, timestamp: number }>()
  private readonly CACHE_DURATION = 2 * 60 * 60 * 1000 // 2ì‹œê°„

  static getInstance(): OptimizedSyncService {
    if (!OptimizedSyncService.instance) {
      OptimizedSyncService.instance = new OptimizedSyncService()
    }
    return OptimizedSyncService.instance
  }

  // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ ë¶„í• 
  private chunkArray<T>(array: T[], chunkSize: number): T[][] {
    const chunks: T[][] = []
    for (let i = 0; i < array.length; i += chunkSize) {
      chunks.push(array.slice(i, i + chunkSize))
    }
    return chunks
  }

  // ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
  private async processBatchesInParallel<T>(
    batches: T[][],
    processor: (batch: T[]) => Promise<void>,
    maxConcurrency: number = 3
  ): Promise<void> {
    const results: Promise<void>[] = []
    
    for (let i = 0; i < batches.length; i += maxConcurrency) {
      const batchGroup = batches.slice(i, i + maxConcurrency)
      const batchPromises = batchGroup.map(batch => processor(batch))
      results.push(...batchPromises)
      
      // ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
      if (results.length >= maxConcurrency) {
        await Promise.all(results.splice(0, maxConcurrency))
      }
    }
    
    // ë‚¨ì€ ë°°ì¹˜ë“¤ ì²˜ë¦¬
    if (results.length > 0) {
      await Promise.all(results)
    }
  }

  // ìµœì í™”ëœ ë°ì´í„° ë³€í™˜ (ë²Œí¬ ì²˜ë¦¬)
  private optimizeDataTransformation(
    sheetData: Record<string, unknown>[],
    columnMapping: { [key: string]: string },
    targetTable: string
  ): Record<string, unknown>[] {
    console.log(`ğŸš€ ë²Œí¬ ë°ì´í„° ë³€í™˜ ì‹œì‘: ${sheetData.length}ê°œ í–‰`)
    
    // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ ë¶„í• 
    const chunkSize = Math.max(100, Math.floor(sheetData.length / 10))
    const chunks = this.chunkArray(sheetData, chunkSize)
    
    const transformedChunks: Record<string, unknown>[][] = []
    
    // ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    chunks.forEach((chunk, index) => {
      const transformedChunk = chunk.map((row, rowIndex) => {
        const transformed: Record<string, unknown> = {}
        
        // ì»¬ëŸ¼ ë§¤í•‘ ì ìš©
        Object.entries(columnMapping).forEach(([sheetColumn, dbColumn]) => {
          if (row[sheetColumn] !== undefined && row[sheetColumn] !== '') {
            transformed[dbColumn] = row[sheetColumn]
          }
        })

        // ë°ì´í„° íƒ€ì… ë³€í™˜
        return this.convertDataTypes(transformed, targetTable)
      })
      
      transformedChunks.push(transformedChunk)
      
      if (index % 5 === 0) {
        console.log(`ğŸ“Š ì²­í¬ ${index + 1}/${chunks.length} ë³€í™˜ ì™„ë£Œ`)
      }
    })
    
    // ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    const result = transformedChunks.flat()
    console.log(`âœ… ë²Œí¬ ë°ì´í„° ë³€í™˜ ì™„ë£Œ: ${result.length}ê°œ í–‰`)
    
    return result
  }

  // ìµœì í™”ëœ ë°ì´í„° íƒ€ì… ë³€í™˜
  private convertDataTypes(data: Record<string, unknown>, tableName: string): Record<string, unknown> {
    const converted = { ...data }

    // ìˆ«ì í•„ë“œ ë³€í™˜
    const numberFields = ['adults', 'child', 'infant', 'total_people', 'price', 'rooms', 'unit_price', 'total_price', 'base_price', 'commission_amount', 'commission_percent']
    numberFields.forEach(field => {
      if (converted[field] !== undefined && converted[field] !== '') {
        converted[field] = parseFloat(String(converted[field])) || 0
      }
    })

    // í…ìŠ¤íŠ¸ í•„ë“œ ë³€í™˜
    const textFields = ['product_id', 'customer_id', 'tour_id', 'id']
    textFields.forEach(field => {
      if (converted[field] !== undefined && converted[field] !== '') {
        converted[field] = String(converted[field])
      }
    })

    // ë¶ˆë¦° í•„ë“œ ë³€í™˜
    const booleanFields = ['is_private_tour']
    booleanFields.forEach(field => {
      if (converted[field] !== undefined && converted[field] !== '') {
        converted[field] = converted[field] === 'TRUE' || converted[field] === 'true' || converted[field] === '1'
      }
    })

    // ë‚ ì§œ í•„ë“œ ë³€í™˜
    let dateFields: string[] = ['tour_date']
    if (tableName === 'tour_hotel_bookings') {
      dateFields = ['event_date', 'check_in_date', 'check_out_date']
    }
    
    dateFields.forEach(field => {
      if (converted[field] && converted[field] !== '') {
        try {
          converted[field] = new Date(String(converted[field])).toISOString().split('T')[0]
        } catch {
          console.warn(`Invalid date format for ${field}:`, converted[field])
        }
      }
    })

    // tour_id ì •ë¦¬
    if (converted.tour_id !== undefined && converted.tour_id !== null) {
      const val = String(converted.tour_id).trim()
      converted.tour_id = val.length === 0 ? null : val
    }

    // ë°°ì—´ í•„ë“œ ë³€í™˜ (PostgreSQL ë°°ì—´ ë¦¬í„°ëŸ´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
    const arrayFields = ['reservation_ids', 'reservations_ids', 'languages']
    arrayFields.forEach(field => {
      if (converted[field] !== undefined && converted[field] !== null) {
        converted[field] = this.convertToPostgreSQLArray(converted[field])
      }
    })

    // í•„ë“œëª… í†µì¼: reservations_ids â†’ reservation_ids
    if (converted.reservations_ids && !converted.reservation_ids) {
      converted.reservation_ids = converted.reservations_ids
      delete converted.reservations_ids
    }

    // JSONB í•„ë“œ ì •ë¦¬
    const jsonbFields = ['selected_options', 'selected_option_prices']
    jsonbFields.forEach(field => {
      if (converted[field] !== undefined && converted[field] !== null) {
        try {
          if (typeof converted[field] === 'string') {
            const parsed = JSON.parse(converted[field])
            converted[field] = parsed
          }
        } catch {
          console.warn(`Invalid JSON format for ${field}:`, converted[field])
          converted[field] = {}
        }
      }
    })

    return converted
  }

  // PostgreSQL ë°°ì—´ ë¦¬í„°ëŸ´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  private convertToPostgreSQLArray(value: unknown): string[] | null {
    if (!value) return null
    
    // ì´ë¯¸ ë°°ì—´ì¸ ê²½ìš°
    if (Array.isArray(value)) {
      return value.map(v => String(v)).filter(v => v.length > 0)
    }
    
    // ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
    if (typeof value === 'string') {
      const trimmed = value.trim()
      
      // JSON ë°°ì—´ í˜•íƒœ: "[\"R1\",\"R2\"]"
      if (trimmed.startsWith('[') && trimmed.endsWith(']')) {
        try {
          const parsed = JSON.parse(trimmed)
          if (Array.isArray(parsed)) {
            return parsed.map(v => String(v)).filter(v => v.length > 0)
          }
        } catch {
          // JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì½¤ë§ˆ êµ¬ë¶„ìœ¼ë¡œ ì²˜ë¦¬
        }
      }
      
      // ì½¤ë§ˆ êµ¬ë¶„ ë¬¸ìì—´: R1, R2, R3 í˜•íƒœ
      return trimmed
        .split(',')
        .map(part => part.trim().replace(/^[\[\"\']+|[\]\\"\']+$/g, ''))
        .filter(part => part.length > 0)
    }
    
    return null
  }

  // ìµœì í™”ëœ ë²Œí¬ upsert (ì„±ëŠ¥ ìµœì í™” í†µí•©)
  private async optimizedBulkUpsert(
    targetTable: string,
    data: Record<string, unknown>[],
    batchSize: number = 200
  ): Promise<{ inserted: number, updated: number, errors: number }> {
    console.log(`ğŸš€ ìµœì í™”ëœ ë²Œí¬ upsert ì‹œì‘: ${data.length}ê°œ í–‰, ë°°ì¹˜ í¬ê¸°: ${batchSize}`)
    
    const results = { inserted: 0, updated: 0, errors: 0 }
    
    try {
      // 1. ë°ì´í„° ê²€ì¦ (ë³‘ë ¬ ì²˜ë¦¬)
      console.log(`ğŸ” ë°ì´í„° ê²€ì¦ ì‹œì‘`)
      const validationRules = this.getValidationRules(targetTable)
      console.log(`ğŸ” ê²€ì¦ ê·œì¹™:`, validationRules)
      
      const { valid, invalid } = await databaseOptimizer.validateBulkData(targetTable, data, validationRules)
      console.log(`âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: ìœ íš¨í•œ í–‰ ${valid.length}ê°œ, ë¬´íš¨í•œ í–‰ ${invalid.length}ê°œ`)
      
      if (invalid.length > 0) {
        console.warn(`âš ï¸ ${invalid.length}ê°œ í–‰ì´ ê²€ì¦ ì‹¤íŒ¨ë¡œ ì œì™¸ë¨`)
        // ì²˜ìŒ 5ê°œ ê²€ì¦ ì‹¤íŒ¨ ì‚¬ìœ  ì¶œë ¥
        invalid.slice(0, 5).forEach((item, index) => {
          console.warn(`ê²€ì¦ ì‹¤íŒ¨ ${index + 1}:`, item.errors)
        })
      }
      
      if (valid.length === 0) {
        console.log('âŒ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.')
        return results
      }
      
      // 2. í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°
      console.log(`ğŸ” í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ: ${targetTable}`)
      const tableColumns = await databaseOptimizer.getTableColumns(targetTable)
      console.log(`âœ… í…Œì´ë¸” ì»¬ëŸ¼ ì •ë³´ ì¡°íšŒ ì™„ë£Œ:`, Array.from(tableColumns))
      
      // 3. ë°°ì¹˜ ì²˜ë¦¬
      const batches = this.chunkArray(valid, batchSize)
      console.log(`ğŸ“Š ë°°ì¹˜ ë¶„í•  ì™„ë£Œ: ${batches.length}ê°œ ë°°ì¹˜`)
      
      // ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ë™ì‹œì„± ì¦ê°€)
      const maxConcurrency = data.length > 5000 ? 5 : 3
      await this.processBatchesInParallel(
        batches,
        async (batch) => {
          try {
            const nowIso = new Date().toISOString()
            
            // IDê°€ ì—†ëŠ” í–‰ë“¤ì— ëŒ€í•´ UUID ìƒì„±
            const preparedBatch = batch.map(row => {
              const prepared = { ...row }
              if (!prepared.id && targetTable !== 'team') {
                prepared.id = this.generateUUID()
              }
              
              // í…Œì´ë¸”ì— updated_at ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
              if (tableColumns.has('updated_at')) {
                prepared.updated_at = nowIso
              }
              
              return prepared
            })
            
            // RLS ì •ì±… ìš°íšŒë¥¼ ìœ„í•œ ì§ì ‘ SQL ì‹¤í–‰
            const { error } = await this.executeDirectUpsert(targetTable, preparedBatch, tableColumns)
            
            if (error) {
              console.error('âŒ ë°°ì¹˜ upsert ì˜¤ë¥˜:', error)
              results.errors += batch.length
            } else {
              results.updated += batch.length
            }
          } catch (error) {
            console.error('âŒ ë°°ì¹˜ upsert ì˜ˆì™¸:', error)
            results.errors += batch.length
          }
        },
        maxConcurrency // ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° 5ê°œ, ê·¸ ì™¸ 3ê°œ ë°°ì¹˜ ë™ì‹œ ì²˜ë¦¬
      )
      
      console.log(`âœ… ë²Œí¬ upsert ì™„ë£Œ: ${results.updated}ê°œ ì—…ë°ì´íŠ¸, ${results.errors}ê°œ ì˜¤ë¥˜`)
      return results
    } catch (error) {
      console.error('âŒ ë²Œí¬ upsert ì „ì²´ ì‹¤íŒ¨:', error)
      results.errors = data.length
      return results
    }
  }

  // í…Œì´ë¸”ë³„ ê²€ì¦ ê·œì¹™ ì •ì˜
  private getValidationRules(tableName: string) {
    const rules: { [key: string]: any } = {
      reservations: {
        requiredFields: [], // í•„ìˆ˜ í•„ë“œ ì—†ìŒ (ëª¨ë“  í•„ë“œ ì„ íƒì )
        foreignKeys: [] // ì™¸ë˜ í‚¤ ê²€ì¦ ë¹„í™œì„±í™” (ì„ íƒì )
      },
      tour_expenses: {
        requiredFields: ['id'],
        foreignKeys: [
          { field: 'tour_id', table: 'tours' },
          { field: 'product_id', table: 'products' }
        ]
      },
      team: {
        requiredFields: ['email']
      },
      reservation_pricing: {
        requiredFields: ['id', 'reservation_id']
      },
      reservation_expenses: {
        requiredFields: ['id']
      },
      company_expenses: {
        requiredFields: ['id']
      }
    }
    
    return rules[tableName] || { requiredFields: [] }
  }

  // UUID ìƒì„±
  private generateUUID(): string {
    if (typeof globalThis !== 'undefined' && globalThis.crypto?.randomUUID) {
      return globalThis.crypto.randomUUID()
    }
    return `${Date.now()}_${Math.random().toString(36).slice(2)}`
  }

  // ë©”ì¸ ìµœì í™”ëœ ë™ê¸°í™” í•¨ìˆ˜
  async optimizedSync(
    spreadsheetId: string,
    sheetName: string,
    targetTable: string,
    columnMapping: { [key: string]: string },
    onProgress?: (event: {
      type: 'start' | 'progress' | 'complete' | 'info' | 'warn' | 'error'
      message?: string
      total?: number
      processed?: number
      inserted?: number
      updated?: number
      errors?: number
    }) => void
  ) {
    try {
      console.log(`ğŸš€ ìµœì í™”ëœ ë™ê¸°í™” ì‹œì‘: ${spreadsheetId}/${sheetName} â†’ ${targetTable}`)
      console.log(`ğŸ“‹ ì»¬ëŸ¼ ë§¤í•‘:`, columnMapping)
      onProgress?.({ type: 'info', message: `ìµœì í™”ëœ ë™ê¸°í™” ì‹œì‘ - ${sheetName} â†’ ${targetTable}` })
      
      // 1. êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì½ê¸°
      console.log(`ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œì‘: ${sheetName}`)
      onProgress?.({ type: 'info', message: 'êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì½ëŠ” ì¤‘...' })
      
      const sheetData = await readSheetDataDynamic(spreadsheetId, sheetName)
      console.log(`ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ì½ê¸° ì™„ë£Œ: ${sheetData.length}ê°œ í–‰`)
      
      if (sheetData.length === 0) {
        console.log(`âš ï¸ ë™ê¸°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.`)
        onProgress?.({ type: 'warn', message: 'ë™ê¸°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.' })
        return { success: true, message: 'No data to sync', count: 0 }
      }
      
      onProgress?.({ type: 'info', message: `êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ${sheetData.length}ê°œ í–‰ì„ ì½ì—ˆìŠµë‹ˆë‹¤.` })
      
      // 2. ìµœì í™”ëœ ë°ì´í„° ë³€í™˜
      console.log(`ğŸ”„ ë°ì´í„° ë³€í™˜ ì‹œì‘`)
      onProgress?.({ type: 'info', message: 'ë°ì´í„° ë³€í™˜ ì¤‘...' })
      const transformedData = this.optimizeDataTransformation(sheetData, columnMapping, targetTable)
      console.log(`âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: ${transformedData.length}ê°œ í–‰`)
      
      // 3. ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
      const optimalBatchSize = this.calculateOptimalBatchSize(transformedData.length)
      console.log(`ğŸ“Š ìµœì  ë°°ì¹˜ í¬ê¸°: ${optimalBatchSize}`)
      onProgress?.({ type: 'info', message: `ìµœì  ë°°ì¹˜ í¬ê¸°: ${optimalBatchSize}` })
      
      // 4. ìµœì í™”ëœ ë²Œí¬ upsert
      console.log(`ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ì‹œì‘`)
      onProgress?.({ type: 'info', message: 'ë°ì´í„°ë² ì´ìŠ¤ì— ë™ê¸°í™” ì‹œì‘...' })
      onProgress?.({ type: 'start', total: transformedData.length })
      
      const results = await this.optimizedBulkUpsert(targetTable, transformedData, optimalBatchSize)
      console.log(`âœ… ë°ì´í„°ë² ì´ìŠ¤ ë™ê¸°í™” ì™„ë£Œ:`, results)
      
      // 5. ê²°ê³¼ ë°˜í™˜
      const summary = {
        success: results.errors === 0,
        message: `ìµœì í™”ëœ ë™ê¸°í™” ì™„ë£Œ: ${results.updated}ê°œ ì—…ë°ì´íŠ¸, ${results.errors}ê°œ ì˜¤ë¥˜`,
        count: transformedData.length,
        details: results
      }
      
      onProgress?.({ type: 'complete' })
      console.log(`âœ… ìµœì í™”ëœ ë™ê¸°í™” ì™„ë£Œ:`, summary)
      
      return summary
      
    } catch (error) {
      console.error('âŒ ìµœì í™”ëœ ë™ê¸°í™” ì˜¤ë¥˜:', error)
      console.error('âŒ ì˜¤ë¥˜ ìŠ¤íƒ:', error instanceof Error ? error.stack : 'No stack trace')
      onProgress?.({ type: 'error', message: `ë™ê¸°í™” ì‹¤íŒ¨: ${error}` })
      return {
        success: false,
        message: `ìµœì í™”ëœ ë™ê¸°í™” ì‹¤íŒ¨: ${error}`,
        count: 0
      }
    }
  }

  // ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚° (ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •)
  // 9500ê°œ ì´ìƒì˜ rows ì²˜ë¦¬ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€
  private calculateOptimalBatchSize(totalRows: number): number {
    if (totalRows > 50000) return 1000
    if (totalRows > 20000) return 800
    if (totalRows > 10000) return 500
    if (totalRows > 5000) return 400
    return 200
  }

  // ìºì‹œ ê´€ë¦¬ (ê³ ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©)
  getFromCache<T>(key: string, ttl?: number): T | null {
    return highPerformanceCache.get<T>(key, ttl)
  }

  setCache<T>(key: string, data: T, ttl?: number): void {
    highPerformanceCache.set(key, data, ttl)
  }

  clearCache(): void {
    highPerformanceCache.clear()
    databaseOptimizer.clearCache()
  }

  // ìºì‹œ í†µê³„
  getCacheStats() {
    return highPerformanceCache.getStats()
  }

  // íŠ¹ì • íŒ¨í„´ì˜ ìºì‹œ ì‚­ì œ
  clearCachePattern(pattern: string): void {
    highPerformanceCache.deletePattern(pattern)
  }

  // RLS ì •ì±…ì„ ìš°íšŒí•˜ëŠ” ì§ì ‘ SQL ì‹¤í–‰
  private async executeDirectUpsert(
    tableName: string, 
    batch: Record<string, unknown>[], 
    tableColumns: Set<string>
  ): Promise<{ error: any }> {
    try {
      // RLSë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ ì„œë¹„ìŠ¤ ê³„ì •ìœ¼ë¡œ ì§ì ‘ upsert ì‹¤í–‰
      const conflictColumn = tableName === 'team' ? 'email' : 'id'
      
      console.log(`ğŸ”§ RLS ìš°íšŒ upsert: ${tableName} í…Œì´ë¸”ì— ${batch.length}ê°œ í–‰ ì²˜ë¦¬`)
      
      // ì„œë²„ í™˜ê²½ì—ì„œëŠ” ì„œë¹„ìŠ¤ ê³„ì • ì‚¬ìš© (RLS ìš°íšŒ)
      const client = supabaseAdmin ?? supabase

      // ì„œë¹„ìŠ¤/ìµëª… í´ë¼ì´ì–¸íŠ¸ë¡œ upsert ì‹¤í–‰
      const { error } = await client
        .from(tableName)
        .upsert(batch, { 
          onConflict: conflictColumn,
          ignoreDuplicates: false
        })
      
      if (error) {
        console.error('RLS bypass upsert error:', error)
        
        // RLS ì˜¤ë¥˜ì¸ ê²½ìš° í´ë°±ìœ¼ë¡œ ê°œë³„ ì²˜ë¦¬
        if (error.code === '42501') {
          console.log(`ğŸ”„ RLS ì˜¤ë¥˜ ê°ì§€ - ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±: ${tableName}`)
          return await this.fallbackIndividualUpsert(tableName, batch, tableColumns)
        }
        
        return { error }
      }
      
      return { error: null }
    } catch (error) {
      console.error('RLS bypass upsert exception:', error)
      return { error }
    }
  }

  // RLS ì˜¤ë¥˜ ì‹œ ë¯¸ë‹ˆ ë°°ì¹˜ ì²˜ë¦¬ í´ë°± (ê°œë³„ ì²˜ë¦¬ ëŒ€ì‹  ì‘ì€ ë°°ì¹˜ë¡œ ì¬ì‹œë„)
  private async fallbackIndividualUpsert(
    tableName: string,
    batch: Record<string, unknown>[],
    tableColumns: Set<string>
  ): Promise<{ error: any }> {
    try {
      // ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ì²˜ë¦¬ (ê°œë³„ ì²˜ë¦¬ ëŒ€ì‹ )
      // ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ëŒ€í­ ì¤„ì´ê¸° ìœ„í•´ ì‘ì€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
      const miniBatchSize = batch.length > 100 ? 25 : 10
      console.log(`ğŸ”„ ë¯¸ë‹ˆ ë°°ì¹˜ í´ë°±: ${tableName} í…Œì´ë¸”ì— ${batch.length}ê°œ í–‰ (ë°°ì¹˜ í¬ê¸°: ${miniBatchSize})`)
      
      const conflictColumn = tableName === 'team' ? 'email' : 'id'
      let successCount = 0
      let errorCount = 0
      const client = supabaseAdmin ?? supabase
      
      // ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
      for (let i = 0; i < batch.length; i += miniBatchSize) {
        const miniBatch = batch.slice(i, i + miniBatchSize)
        
        try {
          const { error } = await client
            .from(tableName)
            .upsert(miniBatch, { 
              onConflict: conflictColumn,
              ignoreDuplicates: false
            })
          
          if (error) {
            // ë¯¸ë‹ˆ ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ë°°ì¹˜ë§Œ ì˜¤ë¥˜ë¡œ ì¹´ìš´íŠ¸ (ë” ì´ìƒ ê°œë³„ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
            console.warn(`ë¯¸ë‹ˆ ë°°ì¹˜ upsert ì˜¤ë¥˜ (${tableName}): ${error.message}`)
            errorCount += miniBatch.length
          } else {
            successCount += miniBatch.length
          }
        } catch (batchError) {
          console.warn(`ë¯¸ë‹ˆ ë°°ì¹˜ upsert ì˜ˆì™¸ (${tableName}):`, batchError)
          errorCount += miniBatch.length
        }
        
        // ë¯¸ë‹ˆ ë°°ì¹˜ ê°„ ìµœì†Œ ì§€ì—° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        if (i + miniBatchSize < batch.length) {
          await new Promise(resolve => setTimeout(resolve, 5))
        }
      }
      
      console.log(`âœ… ë¯¸ë‹ˆ ë°°ì¹˜ í´ë°± ì™„ë£Œ: ${successCount}ê°œ ì„±ê³µ, ${errorCount}ê°œ ì‹¤íŒ¨`)
      
      // ì¼ë¶€ë¼ë„ ì„±ê³µí–ˆìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
      if (successCount > 0) {
        return { error: null }
      } else {
        return { error: new Error(`ëª¨ë“  ë¯¸ë‹ˆ ë°°ì¹˜ upsert ì‹¤íŒ¨: ${errorCount}ê°œ ì˜¤ë¥˜`) }
      }
    } catch (error) {
      console.error('ë¯¸ë‹ˆ ë°°ì¹˜ í´ë°± ì˜ˆì™¸:', error)
      return { error }
    }
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ export
export const optimizedSyncService = OptimizedSyncService.getInstance()
